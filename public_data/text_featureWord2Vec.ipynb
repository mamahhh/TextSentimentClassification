{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  # 每个文本或者句子的截断长度，只保留5000个单词\n",
    "MAX_NUM_WORDS = 28000  # 用于构建词向量的词汇表数量\n",
    "EMBEDDING_DIM = 200  # 词向量维度\n",
    "VALIDATION_SPLIT = 0.16\n",
    "TEST_SPLIT = 0.2\n",
    "GLOVE_DIR = r'D:\\v-yanx\\masijia\\text_mood_classification\\glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing word vectors.\")\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # 单词\n",
    "        coefs = np.asarray(values[1:], dtype='float32')  # 单词对应的向量\n",
    "        embeddings_index[word] = coefs  # 单词及对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = np.load('words_dic.npy',allow_pickle=True).item()\n",
    "x_train = []\n",
    "y_train = []\n",
    "for key in words_train:\n",
    "    sentance = ' '.join(words_train[key])\n",
    "    x_train.append(sentance)\n",
    "    y_train.append(int(key[-1]))\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。就是对文本单词进行去重后\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "# texts_to_sequences(texts) 将多个文档转换为word在词典中索引的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "print(sequences[0])\n",
    "print(len(sequences))  # 24500\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index  # word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "print(\"Founnd %s unique tokens.\" % len(word_index))  # 72955个单词\n",
    "# ['the', 'to', 'of', 'a', 'and', 'in', 'i', 'is', 'that', \"'ax\"] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(list(word_index.keys())[0:10], list(word_index.values())[0:10])  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  # 长度超过MAX_SEQUENCE_LENGTH则截断，不足则补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_train\n",
    "print(\"训练数据大小为：\", data.shape)  # (24500, 500)\n",
    "print(\"标签大小为:\", labels.shape)  # (24500, 1)\n",
    " \n",
    "# 将训练数据划分为训练集和验证集\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)  # 打乱数据\n",
    "data_shuffle = data[indices]\n",
    "labels_shuffle = labels[indices]\n",
    " \n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    " \n",
    "# 训练数据\n",
    "X_train = data_shuffle[:-(num_validation_samples+num_test_samples)]\n",
    "Y_train = labels_shuffle[:-(num_validation_samples+num_test_samples)]\n",
    " \n",
    "# 验证数据\n",
    "x_val = data_shuffle[-(num_validation_samples+num_test_samples):-num_test_samples]\n",
    "y_val = labels_shuffle[-(num_validation_samples+num_test_samples):-num_test_samples]\n",
    "#test data\n",
    "x_test = data_shuffle[-num_test_samples:]\n",
    "y_test = labels_shuffle[-num_test_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备词向量矩阵\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)  # 词汇表数量\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # 28000*200\n",
    " \n",
    "for word, i in word_index.items():\n",
    "    if i>= MAX_NUM_WORDS:  # 过滤掉根据频数排序后排28000以后的词\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)  # 根据词向量字典获取该单词对应的词向量\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# 加载预训练的词向量到Embedding layer\n",
    "embedding_layer = Embedding(input_dim=num_words,  # 词汇表单词数量\n",
    "                            output_dim=EMBEDDING_DIM,  # 词向量维度\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,  # 文本或者句子截断长度\n",
    "                            trainable=False)  # 词向量矩阵不进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    " \n",
    "def scheduler(epoch):\n",
    "    # 每隔100个epoch，学习率减小为原来的1/10\n",
    "    if epoch % 1 == 0 and epoch != 0:\n",
    "        lr = K.get_value(model.optimizer.lr)\n",
    "        K.set_value(model.optimizer.lr, lr * 0.8)\n",
    "        print(\"lr changed to {}\".format(lr * 0.8))\n",
    "    return K.get_value(model.optimizer.lr)\n",
    " \n",
    "reduce_lr = LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"开始训练模型.....\")\n",
    "# 使用\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500，也就是模型的输入为batch_size*1000\n",
    "embedded_sequences = embedding_layer(sequence_input)  # 返回batch_size*500*200\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)  # 输出的神经元个数为128，卷积的窗口大小为5\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    " \n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    " \n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))\n",
    "model.summary()\n",
    "model.save(\"textClassifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"开始训练模型.....\")\n",
    "# 使用\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500，也就是模型的输入为batch_size*1000\n",
    "embedded_sequences = embedding_layer(sequence_input)  # 返回batch_size*500*200\n",
    "x = Dropout(0.2)(embedded_sequences)\n",
    "x = Conv1D(32, 5, activation='relu',kernel_initializer=glorot_normal(seed=None))(x)  # 输出的神经元个数为128，卷积的窗口大小为5\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(32, 3, activation='relu',kernel_initializer=glorot_normal(seed=None))(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(32, activation='relu',kernel_initializer=glorot_normal(seed=None))(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    " \n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    " \n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "model.save(\"textClassifier.h5\")\n",
    "print(model.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
