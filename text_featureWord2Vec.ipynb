{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import glorot_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  # 每个文本或者句子的截断长度，只保留500个单词\n",
    "MAX_NUM_WORDS = 28000  # 用于构建词向量的词汇表数量\n",
    "EMBEDDING_DIM = 200  # 词向量维度\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "GLOVE_DIR = r'D:\\v-yanx\\masijia\\text_mood_classification\\glove.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Indexing word vectors.\")\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # 单词\n",
    "        coefs = np.asarray(values[1:], dtype='float32')  # 单词对应的向量\n",
    "        embeddings_index[word] = coefs  # 单词及对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_train = np.load('words_dic.npy',allow_pickle=True).item()\n",
    "x_train = []\n",
    "y_train = []\n",
    "for key in words_train:\n",
    "    sentance = ' '.join(words_train[key])\n",
    "    x_train.append(sentance)\n",
    "    y_train.append(int(key[-1]))\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test = np.load('test_words_dic.npy',allow_pickle=True).item()\n",
    "x_test = []\n",
    "for key in words_test:\n",
    "    sentance = ' '.join(words_test[key])\n",
    "    x_test.append(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 47, 5, 29, 1, 1501, 737, 56, 208, 10, 28, 124, 109, 1730, 8522, 6, 3, 1228, 25, 2846, 9, 8978, 1482, 2, 296, 9, 200, 27, 398, 6, 337, 1208, 2, 337, 624, 10, 807, 12, 251, 97, 844, 145, 11, 2, 24, 457, 67, 3, 123, 105, 349, 8, 13, 163]\n",
      "24500\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。就是对文本单词进行去重后\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "# texts_to_sequences(texts) 将多个文档转换为word在词典中索引的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "t_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "print(sequences[0])\n",
    "print(len(sequences))  # 24500\n",
    "print(len(t_sequences))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founnd 72955 unique tokens.\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in', 'i'] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index  # word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "print(\"Founnd %s unique tokens.\" % len(word_index))  # 72955个单词\n",
    "# ['the', 'to', 'of', 'a', 'and', 'in', 'i', 'is', 'that', \"'ax\"] [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(list(word_index.keys())[0:10], list(word_index.values())[0:10])  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  # 长度超过MAX_SEQUENCE_LENGTH则截断，不足则补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = pad_sequences(t_sequences, maxlen=MAX_SEQUENCE_LENGTH)  # 长度超过MAX_SEQUENCE_LENGTH则截断，不足则补0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22000, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据大小为： (24500, 500)\n",
      "标签大小为: (24500,)\n"
     ]
    }
   ],
   "source": [
    "labels = y_train\n",
    "print(\"训练数据大小为：\", data.shape)  # (24500, 500)\n",
    "print(\"标签大小为:\", labels.shape)  # (24500, 1)\n",
    " \n",
    "# 将训练数据划分为训练集和验证集\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(indices)  # 打乱数据\n",
    "data_shuffle = data[indices]\n",
    "labels_shuffle = labels[indices]\n",
    " \n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "#num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    " \n",
    "# 训练数据\n",
    "X_train = data_shuffle[:-num_validation_samples]\n",
    "Y_train = labels_shuffle[:-num_validation_samples]\n",
    " \n",
    "# 验证数据\n",
    "x_val = data_shuffle[-num_validation_samples:]\n",
    "y_val = labels_shuffle[-num_validation_samples:]\n",
    "#test data\n",
    "# x_test = data_shuffle[-num_test_samples:]\n",
    "# y_test = labels_shuffle[-num_test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5042857142857143\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "for i in y_val:\n",
    "    if i==1:\n",
    "        pos+=1\n",
    "print(pos/y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备词向量矩阵\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)  # 词汇表数量\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # 28000*200\n",
    " \n",
    "for word, i in word_index.items():\n",
    "    if i>= MAX_NUM_WORDS:  # 过滤掉根据频数排序后排28000以后的词\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)  # 根据词向量字典获取该单词对应的词向量\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "# 加载预训练的词向量到Embedding layer\n",
    "embedding_layer = Embedding(input_dim=num_words,  # 词汇表单词数量\n",
    "                            output_dim=EMBEDDING_DIM,  # 词向量维度\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,  # 文本或者句子截断长度\n",
    "                            trainable=False)  # 词向量矩阵不进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 496, 32)           32032     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 5,632,577\n",
      "Trainable params: 32,577\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_MLP'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500，也就是模型的输入为batch_size*500\n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "\n",
    "x = Conv1D(32, 5, activation='relu',kernel_initializer=glorot_normal(seed=10))(embedded_sequences)  # 输出的神经元个数为32，卷积的窗口大小为5\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(16, activation='relu',kernel_initializer=glorot_normal(seed=10))(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/10\n",
      "19600/19600 [==============================] - 8s 433us/sample - loss: 0.1460 - acc: 0.9453 - val_loss: 0.5421 - val_acc: 0.8067\n",
      "Epoch 2/10\n",
      "19600/19600 [==============================] - 7s 368us/sample - loss: 0.1257 - acc: 0.9541 - val_loss: 0.3723 - val_acc: 0.8551\n",
      "Epoch 3/10\n",
      "19600/19600 [==============================] - 7s 370us/sample - loss: 0.1156 - acc: 0.9593 - val_loss: 0.3684 - val_acc: 0.8629\n",
      "Epoch 4/10\n",
      "19600/19600 [==============================] - 7s 366us/sample - loss: 0.1057 - acc: 0.9637 - val_loss: 0.4015 - val_acc: 0.8569\n",
      "Epoch 5/10\n",
      "19600/19600 [==============================] - 7s 368us/sample - loss: 0.0921 - acc: 0.9673 - val_loss: 0.5893 - val_acc: 0.8118\n",
      "Epoch 6/10\n",
      "19600/19600 [==============================] - 7s 366us/sample - loss: 0.0849 - acc: 0.9708 - val_loss: 0.4038 - val_acc: 0.8641\n",
      "Epoch 7/10\n",
      "19600/19600 [==============================] - 7s 369us/sample - loss: 0.0813 - acc: 0.9708 - val_loss: 0.4532 - val_acc: 0.8512\n",
      "Epoch 8/10\n",
      "19600/19600 [==============================] - 7s 366us/sample - loss: 0.0727 - acc: 0.9762 - val_loss: 0.4485 - val_acc: 0.8545\n",
      "Epoch 9/10\n",
      "19600/19600 [==============================] - 7s 373us/sample - loss: 0.0642 - acc: 0.9807 - val_loss: 0.5925 - val_acc: 0.8345\n",
      "Epoch 10/10\n",
      "19600/19600 [==============================] - 7s 370us/sample - loss: 0.0578 - acc: 0.9811 - val_loss: 0.6024 - val_acc: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xa994568d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=10, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640816\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_MLP\\model_06-0.8641_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_model_06-0.8641_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 496, 16)           16016     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 5,616,305\n",
      "Trainable params: 16,305\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_MLP\\2'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500，也就是模型的输入为batch_size*500\n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = Conv1D(16, 5, activation='relu',kernel_initializer=glorot_normal(seed=10))(embedded_sequences)  # 输出的神经元个数为16，卷积的窗口大小为5\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(16, activation='relu',kernel_initializer=glorot_normal(seed=10))(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 8s 428us/sample - loss: 0.5225 - acc: 0.7386 - val_loss: 0.5011 - val_acc: 0.7471\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.3657 - acc: 0.8397 - val_loss: 0.3933 - val_acc: 0.8249\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 7s 367us/sample - loss: 0.3174 - acc: 0.8654 - val_loss: 0.4726 - val_acc: 0.7882\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 7s 360us/sample - loss: 0.2896 - acc: 0.8793 - val_loss: 0.3316 - val_acc: 0.8600\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.2667 - acc: 0.8895 - val_loss: 0.4067 - val_acc: 0.8231\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 7s 359us/sample - loss: 0.2475 - acc: 0.8977 - val_loss: 0.3335 - val_acc: 0.8602\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 7s 363us/sample - loss: 0.2291 - acc: 0.9091 - val_loss: 0.3818 - val_acc: 0.8386\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 7s 361us/sample - loss: 0.2153 - acc: 0.9148 - val_loss: 0.4206 - val_acc: 0.8278\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 7s 363us/sample - loss: 0.1988 - acc: 0.9221 - val_loss: 0.4335 - val_acc: 0.8251\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.1907 - acc: 0.9243 - val_loss: 0.3523 - val_acc: 0.8582\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 7s 364us/sample - loss: 0.1746 - acc: 0.9343 - val_loss: 0.3748 - val_acc: 0.8547\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 7s 363us/sample - loss: 0.1671 - acc: 0.9346 - val_loss: 0.4413 - val_acc: 0.8363\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.1515 - acc: 0.9443 - val_loss: 0.5519 - val_acc: 0.7992\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.1464 - acc: 0.9452 - val_loss: 0.6645 - val_acc: 0.7739\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 7s 362us/sample - loss: 0.1359 - acc: 0.9510 - val_loss: 0.4178 - val_acc: 0.8433\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 7s 360us/sample - loss: 0.1305 - acc: 0.9519 - val_loss: 0.4029 - val_acc: 0.8508\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 7s 364us/sample - loss: 0.1197 - acc: 0.9556 - val_loss: 0.5014 - val_acc: 0.8290\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 7s 361us/sample - loss: 0.1165 - acc: 0.9581 - val_loss: 0.4296 - val_acc: 0.8516\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 7s 361us/sample - loss: 0.1071 - acc: 0.9617 - val_loss: 0.4496 - val_acc: 0.8463\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 7s 363us/sample - loss: 0.1003 - acc: 0.9658 - val_loss: 0.5250 - val_acc: 0.8412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64b9ca7da0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602041\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_MLP\\2\\model_06-0.8602_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_CNN2_06-0.8602_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 5,921,001\n",
      "Trainable params: 321,001\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500\n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = LSTM(200, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)  \n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.6223 - acc: 0.6533 - val_loss: 0.4872 - val_acc: 0.7680\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.5338 - acc: 0.7403 - val_loss: 0.4988 - val_acc: 0.7655\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.4747 - acc: 0.7806 - val_loss: 0.4105 - val_acc: 0.8190\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.4116 - acc: 0.8201 - val_loss: 0.4067 - val_acc: 0.8435\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.3711 - acc: 0.8376 - val_loss: 0.3322 - val_acc: 0.8578\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.3442 - acc: 0.8535 - val_loss: 0.3336 - val_acc: 0.8624\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.3207 - acc: 0.8665 - val_loss: 0.3991 - val_acc: 0.8155\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.3125 - acc: 0.8685 - val_loss: 0.4269 - val_acc: 0.8155\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2929 - acc: 0.8789 - val_loss: 0.3362 - val_acc: 0.8524\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2786 - acc: 0.8861 - val_loss: 0.3318 - val_acc: 0.8569\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2670 - acc: 0.8920 - val_loss: 0.2825 - val_acc: 0.8829\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2563 - acc: 0.8947 - val_loss: 0.3196 - val_acc: 0.8635\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2671 - acc: 0.8909 - val_loss: 0.3230 - val_acc: 0.8590\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2356 - acc: 0.9038 - val_loss: 0.3213 - val_acc: 0.8669\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 124s 6ms/sample - loss: 0.2248 - acc: 0.9097 - val_loss: 0.3312 - val_acc: 0.8724\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.2102 - acc: 0.9171 - val_loss: 0.2928 - val_acc: 0.8812\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.1965 - acc: 0.9213 - val_loss: 0.3231 - val_acc: 0.8678\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 124s 6ms/sample - loss: 0.1895 - acc: 0.9269 - val_loss: 0.3385 - val_acc: 0.8714\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.1777 - acc: 0.9291 - val_loss: 0.2881 - val_acc: 0.8831\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 123s 6ms/sample - loss: 0.1656 - acc: 0.9353 - val_loss: 0.2989 - val_acc: 0.8831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64b26c2d30>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88306123\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\model_20-0.8831_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_LSTM200_20-0.8831_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 350)               771400    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 350)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 351       \n",
      "=================================================================\n",
      "Total params: 6,371,751\n",
      "Trainable params: 771,751\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\2'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500\n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = LSTM(350, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)  \n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 156s 8ms/sample - loss: 0.6428 - acc: 0.6368 - val_loss: 0.6759 - val_acc: 0.6541\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 155s 8ms/sample - loss: 0.5572 - acc: 0.7243 - val_loss: 0.4868 - val_acc: 0.7782\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 152s 8ms/sample - loss: 0.4925 - acc: 0.7688 - val_loss: 0.7062 - val_acc: 0.6573\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 152s 8ms/sample - loss: 0.4339 - acc: 0.8041 - val_loss: 0.4082 - val_acc: 0.8294\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 153s 8ms/sample - loss: 0.4017 - acc: 0.8235 - val_loss: 0.6476 - val_acc: 0.7155\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.3577 - acc: 0.8483 - val_loss: 0.3399 - val_acc: 0.8643\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 152s 8ms/sample - loss: 0.3367 - acc: 0.8551 - val_loss: 0.3209 - val_acc: 0.8620\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 152s 8ms/sample - loss: 0.3153 - acc: 0.8686 - val_loss: 0.3317 - val_acc: 0.8657\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2961 - acc: 0.8758 - val_loss: 0.4702 - val_acc: 0.8184\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2819 - acc: 0.8824 - val_loss: 0.3615 - val_acc: 0.8235\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2654 - acc: 0.8909 - val_loss: 0.3773 - val_acc: 0.8329\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2503 - acc: 0.8983 - val_loss: 0.3062 - val_acc: 0.8692\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2386 - acc: 0.9020 - val_loss: 0.2927 - val_acc: 0.8794\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2225 - acc: 0.9099 - val_loss: 0.3512 - val_acc: 0.8600\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.2118 - acc: 0.9158 - val_loss: 0.3614 - val_acc: 0.8639\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 155s 8ms/sample - loss: 0.1930 - acc: 0.9222 - val_loss: 0.4710 - val_acc: 0.8298\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.1753 - acc: 0.9295 - val_loss: 0.3270 - val_acc: 0.8688\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 155s 8ms/sample - loss: 0.1614 - acc: 0.9367 - val_loss: 0.3236 - val_acc: 0.8796\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 155s 8ms/sample - loss: 0.1496 - acc: 0.9416 - val_loss: 0.3259 - val_acc: 0.8831\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 154s 8ms/sample - loss: 0.1364 - acc: 0.9484 - val_loss: 0.3251 - val_acc: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64b926f6d8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88306123\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\2\\model_19-0.8831_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_LSTM350_19-0.8831_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_30 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,720,501\n",
      "Trainable params: 120,501\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\3'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  \n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)  # 输出的神经元个数为100\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 137s 7ms/sample - loss: 0.6193 - acc: 0.6541 - val_loss: 0.6448 - val_acc: 0.6331\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.5404 - acc: 0.7325 - val_loss: 0.6272 - val_acc: 0.6663\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.4854 - acc: 0.7757 - val_loss: 0.4179 - val_acc: 0.8120\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 131s 7ms/sample - loss: 0.4438 - acc: 0.7973 - val_loss: 0.4157 - val_acc: 0.8178\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 133s 7ms/sample - loss: 0.3975 - acc: 0.8254 - val_loss: 0.3523 - val_acc: 0.8380\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.3689 - acc: 0.8404 - val_loss: 0.3369 - val_acc: 0.8492\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.3432 - acc: 0.8536 - val_loss: 0.3448 - val_acc: 0.8522\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.3263 - acc: 0.8647 - val_loss: 0.3380 - val_acc: 0.8512\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 133s 7ms/sample - loss: 0.3129 - acc: 0.8690 - val_loss: 0.3174 - val_acc: 0.8600\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 131s 7ms/sample - loss: 0.2994 - acc: 0.8780 - val_loss: 0.3434 - val_acc: 0.8512\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2887 - acc: 0.8810 - val_loss: 0.2982 - val_acc: 0.8786\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.2768 - acc: 0.8869 - val_loss: 0.3152 - val_acc: 0.8771\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.2689 - acc: 0.8914 - val_loss: 0.3437 - val_acc: 0.8533\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 134s 7ms/sample - loss: 0.2588 - acc: 0.8966 - val_loss: 0.3405 - val_acc: 0.8602\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 130s 7ms/sample - loss: 0.2534 - acc: 0.8960 - val_loss: 0.2814 - val_acc: 0.8841\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2469 - acc: 0.8993 - val_loss: 0.2856 - val_acc: 0.8837\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2342 - acc: 0.9075 - val_loss: 0.2739 - val_acc: 0.8884\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2246 - acc: 0.9102 - val_loss: 0.2873 - val_acc: 0.8786\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2197 - acc: 0.9113 - val_loss: 0.4277 - val_acc: 0.8282\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 132s 7ms/sample - loss: 0.2117 - acc: 0.9180 - val_loss: 0.3202 - val_acc: 0.8722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xa99c5f9e8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88836735\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\3\\model_17-0.8884_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_LSTM100_17-0.8884_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_31 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 250)               451000    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 6,051,251\n",
      "Trainable params: 451,251\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\250'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  \n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = LSTM(250, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)  \n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 142s 7ms/sample - loss: 0.6282 - acc: 0.6495 - val_loss: 0.6923 - val_acc: 0.5835\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 139s 7ms/sample - loss: 0.5436 - acc: 0.7337 - val_loss: 0.4728 - val_acc: 0.7867\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 139s 7ms/sample - loss: 0.4828 - acc: 0.7804 - val_loss: 0.4439 - val_acc: 0.7996\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 138s 7ms/sample - loss: 0.4341 - acc: 0.8041 - val_loss: 0.4953 - val_acc: 0.7731\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 137s 7ms/sample - loss: 0.3985 - acc: 0.8260 - val_loss: 0.5411 - val_acc: 0.7345\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 136s 7ms/sample - loss: 0.3707 - acc: 0.8404 - val_loss: 0.4535 - val_acc: 0.7992\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 139s 7ms/sample - loss: 0.3479 - acc: 0.8509 - val_loss: 0.3949 - val_acc: 0.8282\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 138s 7ms/sample - loss: 0.3248 - acc: 0.8610 - val_loss: 0.3973 - val_acc: 0.8476\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 139s 7ms/sample - loss: 0.3119 - acc: 0.8693 - val_loss: 0.3049 - val_acc: 0.8718\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 138s 7ms/sample - loss: 0.2951 - acc: 0.8783 - val_loss: 0.3569 - val_acc: 0.8551\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 138s 7ms/sample - loss: 0.2823 - acc: 0.8841 - val_loss: 0.4040 - val_acc: 0.8318\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 135s 7ms/sample - loss: 0.2700 - acc: 0.8889 - val_loss: 0.3420 - val_acc: 0.8469\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 137s 7ms/sample - loss: 0.2583 - acc: 0.8934 - val_loss: 0.2901 - val_acc: 0.8851\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 137s 7ms/sample - loss: 0.2498 - acc: 0.8983 - val_loss: 0.3373 - val_acc: 0.8620\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 138s 7ms/sample - loss: 0.2346 - acc: 0.9059 - val_loss: 0.2887 - val_acc: 0.8902\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.2191 - acc: 0.9103 - val_loss: 0.3188 - val_acc: 0.8753\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.2077 - acc: 0.9159 - val_loss: 0.3197 - val_acc: 0.8733\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.1962 - acc: 0.9207 - val_loss: 0.3200 - val_acc: 0.8743\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.1858 - acc: 0.9264 - val_loss: 0.2856 - val_acc: 0.8869\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 126s 6ms/sample - loss: 0.1747 - acc: 0.9305 - val_loss: 0.4112 - val_acc: 0.8437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64c3164320>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8902041\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\250\\model_15-0.8902_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_LSTM250_15-0.8902_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 150)               210600    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 5,810,751\n",
      "Trainable params: 210,751\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "save_dir = r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\150'\n",
    "weight_path = 'model_{epoch:02d}-{val_acc:.4f}_glorot_normal.hdf5'\n",
    "checkpoints = ModelCheckpoint(os.path.join(save_dir,weight_path), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='max')\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') \n",
    "embedded_sequences = embedding_layer(sequence_input)  \n",
    "x = LSTM(150, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)  \n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19600 samples, validate on 4900 samples\n",
      "Epoch 1/20\n",
      "19600/19600 [==============================] - 125s 6ms/sample - loss: 0.6206 - acc: 0.6553 - val_loss: 0.5224 - val_acc: 0.7324\n",
      "Epoch 2/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.5434 - acc: 0.7305 - val_loss: 0.4752 - val_acc: 0.7900\n",
      "Epoch 3/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.4855 - acc: 0.7736 - val_loss: 0.4135 - val_acc: 0.8149\n",
      "Epoch 4/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.4414 - acc: 0.8029 - val_loss: 0.4176 - val_acc: 0.8029\n",
      "Epoch 5/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.3930 - acc: 0.8262 - val_loss: 0.5423 - val_acc: 0.7022\n",
      "Epoch 6/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.3655 - acc: 0.8403 - val_loss: 0.3694 - val_acc: 0.8422\n",
      "Epoch 7/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.3389 - acc: 0.8576 - val_loss: 0.4108 - val_acc: 0.8039\n",
      "Epoch 8/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.3211 - acc: 0.8641 - val_loss: 0.3289 - val_acc: 0.8600\n",
      "Epoch 9/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.3033 - acc: 0.8708 - val_loss: 0.4544 - val_acc: 0.7876\n",
      "Epoch 10/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2932 - acc: 0.8797 - val_loss: 0.3052 - val_acc: 0.8722\n",
      "Epoch 11/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2763 - acc: 0.8865 - val_loss: 0.3114 - val_acc: 0.8641\n",
      "Epoch 12/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2677 - acc: 0.8922 - val_loss: 0.3265 - val_acc: 0.8592\n",
      "Epoch 13/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.2561 - acc: 0.8953 - val_loss: 0.2854 - val_acc: 0.8808\n",
      "Epoch 14/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2479 - acc: 0.8992 - val_loss: 0.3065 - val_acc: 0.8751\n",
      "Epoch 15/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2407 - acc: 0.8997 - val_loss: 0.3459 - val_acc: 0.8522\n",
      "Epoch 16/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2236 - acc: 0.9095 - val_loss: 0.3046 - val_acc: 0.8800\n",
      "Epoch 17/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2188 - acc: 0.9114 - val_loss: 0.3565 - val_acc: 0.8604\n",
      "Epoch 18/20\n",
      "19600/19600 [==============================] - 122s 6ms/sample - loss: 0.2080 - acc: 0.9164 - val_loss: 0.2835 - val_acc: 0.8810\n",
      "Epoch 19/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.2021 - acc: 0.9183 - val_loss: 0.2916 - val_acc: 0.8820\n",
      "Epoch 20/20\n",
      "19600/19600 [==============================] - 121s 6ms/sample - loss: 0.1907 - acc: 0.9237 - val_loss: 0.3098 - val_acc: 0.8761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x64c7ddfbe0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "model.fit(X_train ,Y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val),callbacks=[checkpoints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8820408\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\v-yanx\\masijia\\text_mood_classification\\text_mood_classification\\word2vec_LSTM\\150\\model_19-0.8820_glorot_normal.hdf5')\n",
    "loss,acc = model.evaluate(x_val, y_val,batch_size=128,verbose=0)\n",
    "print(acc)\n",
    "prediction = model.predict(X_test)\n",
    "np.savetxt(r'submit\\word2vec\\submission_LSTM150_19-0.8820_glorot_normal.txt',prediction, fmt='%.9f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 500, 200)          5600000   \n",
      "=================================================================\n",
      "Total params: 5,600,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 5,600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')  # 返回一个张量，长度为500\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "model = Model(sequence_input, embedded_sequences)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500, 500, 200)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model.predict(data)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据大小为： (24500, 500, 200)\n",
      "标签大小为: (24500,)\n",
      "(19600, 500, 200)\n"
     ]
    }
   ],
   "source": [
    "labels = y_train\n",
    "print(\"训练数据大小为：\", X.shape)  # (24500, 500)\n",
    "print(\"标签大小为:\", labels.shape)  # (24500, 1)\n",
    " \n",
    "# 将训练数据划分为训练集和验证集\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(indices)  # 打乱数据\n",
    "X_shuffle = X[indices]\n",
    "labels_shuffle = labels[indices]\n",
    " \n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "#num_test_samples = int(TEST_SPLIT * data.shape[0])\n",
    " \n",
    "# 训练数据\n",
    "X_train = X_shuffle[:-num_validation_samples]\n",
    "Y_train = labels_shuffle[:-num_validation_samples]\n",
    "print(X_train.shape)\n",
    " \n",
    "# 验证数据\n",
    "x_val = X_shuffle[-num_validation_samples:]\n",
    "y_val = labels_shuffle[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19600, 500)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.sum(X_train,axis = 1)\n",
    "\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\v-yanx\\anaconda3\\envs\\captcha\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8071428571428572\n",
      "0.8131632653061225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                               multi_class='ovr').fit(np.sum(X_train,axis = 1)/500, Y_train)\n",
    "y_pred = clf.predict(np.sum(x_val,axis = 1)/500)\n",
    "y_pred_prob = clf.predict_proba(np.sum(x_val,axis = 1)/500) \n",
    "acc = clf.score(np.sum(x_val,axis = 1)/500, y_val)\n",
    "print(acc)\n",
    "acc = clf.score(np.sum(X_train,axis = 1)/500, Y_train)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\v-yanx\\anaconda3\\envs\\captcha\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656530612244898\n",
      "0.6657142857142857\n"
     ]
    }
   ],
   "source": [
    "# rbf SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(np.sum(X_train,axis = 1)/500, Y_train) \n",
    "y_pred = clf.predict(np.sum(x_val,axis = 1)/500)\n",
    "acc = clf.score(np.sum(x_val,axis = 1)/500, y_val)\n",
    "print(acc)\n",
    "acc = clf.score(np.sum(X_train,axis = 1)/500, Y_train)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\v-yanx\\anaconda3\\envs\\captcha\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4982142857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(np.sum(X_train,axis = 1)/X_train.shape[0], Y_train) \n",
    "y_pred = clf.predict(np.sum(x_val,axis = 1)/x_val.shape[0])\n",
    "acc = clf.score(np.sum(x_val,axis = 1)/x_val.shape[0], y_val)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
